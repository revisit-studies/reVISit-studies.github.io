"use strict";(globalThis.webpackChunkdocs_website=globalThis.webpackChunkdocs_website||[]).push([[38749],{91895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/11/10/ieee-vis-award","metadata":{"permalink":"/blog/2025/11/10/ieee-vis-award","source":"@site/blog/2025-11-10-ieee-vis-award.md","title":"ReVISit 2 Paper Wins Best Paper Award at IEEE VIS","description":"We\u2019re happy to share that our paper on ReVISit 2 received a Best Paper Award at IEEE VIS 2025. ReVISit 2 is an open framework for designing, deploying, and disseminating browser-based visualization studies across the full experiment life cycle. In this post, we summarize the contributions of the paper, describe the replication studies that demonstrate the system\u2019s capabilities, reflect on feedback from users, and outline how reVISit can support more reproducible and expressive experimental research.","date":"2025-11-10T00:00:00.000Z","tags":[],"readingTime":5.08,"hasTruncateMarker":true,"authors":[{"name":"The ReVISit Team","key":"team","page":null}],"frontMatter":{"layout":"post","title":"ReVISit 2 Paper Wins Best Paper Award at IEEE VIS","authors":["team"]},"unlisted":false,"nextItem":{"title":"ReVISit v2.3: Databases, Screen Capture, Qualitative Analysis, Styling, and More!","permalink":"/blog/2025/10/27/release-2.3"}},"content":"We\u2019re happy to share that our paper on ReVISit 2 received a Best Paper Award at IEEE VIS 2025. ReVISit 2 is an open framework for designing, deploying, and disseminating browser-based visualization studies across the full experiment life cycle. In this post, we summarize the contributions of the paper, describe the replication studies that demonstrate the system\u2019s capabilities, reflect on feedback from users, and outline how reVISit can support more reproducible and expressive experimental research.\\n\\n\\n![Selfie by the reVISit team on stage at IEEE VIS in Vienna after receiving the award.](/img/blog_posts/2026_award_photo.jpg)\\n\x3c!-- truncate --\x3e\\n\\nRunning online visualization studies is now standard practice in VIS and HCI research. Yet the process remains fragmented: researchers stitch together survey tools, custom web code, logging scripts, analysis pipelines, and ad hoc debugging workflows. Users of [reVISit](https://revisit.dev) already know this story: reVISit consolidates this ecosystem into a single open framework that supports the **entire experiment life cycle** \u2013 from design to dissemination.  \\n\\nTo inform the academic community about the new developments in reVISit 2 \u2013 which we already described in these [blog](https://revisit.dev/blog/2025/01/20/release-2.0/  \\n) [posts](https://revisit.dev/blog/2025/10/27/release-2.3/)  we wrote an [academic paper](https://www.visdesignlab.net/publications/2025_vis_revisit/) about it.\\n\\n## Positioning reVISit in the Ecosystem\\n\\nIn the paper, we first situate reVISit among existing study platforms. We compare it to commercial survey systems, domain-specific research tools, and library-based frameworks. While survey platforms excel at rapid deployment, they rarely support sophisticated interaction logging or fine-grained experimental control. Academic tools often address specific domains or slices of the workflow, but lack long-term maintainability or broad adoption.\\n\\nReVISit 2 is designed differently: it treats experiment design as programmable infrastructure. A JSON-based domain-specific language (DSL) models sequences, blocks, counterbalancing strategies, interruptions, skip logic, and dynamic control flow. On top of that, reVISitPy provides Python bindings that allow researchers to generate complex study configurations directly from notebooks. **The result is a framework that emphasizes expressiveness, reproducibility, and ownership over one\u2019s experimental stack.**\\n\\nWe also describe technical advances in reVISit 2, including first-class Vega support, automated provenance tracking, participant replay, and improved debugging tools such as the study browser. These features aim to tighten feedback loops during piloting while preserving transparency during dissemination.\\n\\n\\n## Putting it to the Test: Replication Studies\\n\\nTo demonstrate that these capabilities are not merely architectural, we conducted a series of replication studies.\\n\\n![Screenshot of the revisit analysis interface showing a replay of an interactive study with think aloud and provenance tracking. A large bubble chart is the main stimulus.](/img/blog_posts/2026_bubble_chart.png)\\n\\n\\nEach study highlights a different capability of the system. In one, we implement adaptive and staircase-style designs to [evaluate visualizations of correlations](https://revisit.dev/replication-studies/?tab=JND) using [dynamic sequencing logic](https://revisit.dev/docs/typedoc/interfaces/DynamicBlock/), showing how complex control flow can be expressed directly in the study configuration. In another, we integrate [think-aloud protocols](https://revisit.dev/docs/designing-studies/think-aloud/) by embedding audio recording and transcription into browser-based experiments, allowing [researchers to capture reasoning during interaction](https://revisit.dev/replication-studies/?tab=Search) rather than only after the fact. Finally, we demonstrate [provenance tracking and replay](https://revisit.dev/replication-studies/?tab=Pattern) by instrumenting interactive visualizations to capture detailed interaction histories, enabling fine-grained participant replay and qualitative analysis. reVISit 2 also provides deep linking to specific trials or moments in user studies, to aid in dissemination and transparency. For example, [this link](https://revisit.dev/replication-studies/bubblechart-study/LzE2MTl4ZVRMTk5nSFlNYmd1ZDhjZz09?participantId=936e6c58-fc6e-4e1f-9af9-9c9ce2a65952) takes you to the exact state you see in the above image. \\n\\nAcross these replications, we recruited hundreds of participants and reproduced key findings from prior visualization studies. Just as importantly, the studies surfaced practical lessons about counterbalancing, recruitment logistics, and ongoing challenges in deploying sophisticated designs online. The replication work serves as both validation and stress test for the framework, and as real live examples for studies (including the data) for the community to learn from. \\n\\nAcross these replications, we recruited hundreds of participants and reproduced key findings from prior visualization studies. Just as importantly, the studies surfaced practical lessons about counterbalancing, recruitment logistics, and the realities of deploying sophisticated designs online. Together, they function both as validation and as a stress test of the framework. **They also remain publicly accessible \u2013 complete with study configurations and data \u2013 serving as concrete, real-world examples that the community can inspect, reuse, and learn from.**\\n\\n\\n## What Users Told Us\\n\\nWe also interviewed experienced reVISit users to better understand how the system performs in practice. Several themes emerged:\\n\\n- **Tighter development loops.** Users appreciated the integrated development environment and the study browser, which makes it possible to jump directly to specific trials without stepping through an entire study.\\n- **Expressiveness over convenience.** While the DSL requires programming knowledge, users valued the flexibility it affords \u2013 especially for mixed designs and adaptive sequencing.\\n- **Learning curve trade-offs.** ReVISit inherits complexity from modern web tooling (e.g., React, TypeScript). This can be a barrier for less technical researchers, but it also enables deeper customization and extensibility.\\n- **Open infrastructure.** The ability to fork studies, inspect core code, and maintain version stability was frequently cited as a strength, particularly for reproducibility.\\n\\nOverall, feedback confirmed that reVISit is most effective for technically oriented research teams who need more than a survey builder.\\n\\n## Recognition at IEEE VIS\\n\\nWe were very honored to receive an [**IEEE VIS Best Paper Award**](https://ieeevis.org/year/2025/info/awards/best-paper-awards#:~:text=ReVISit%202%3A%20A%20Full%20Experiment%20Life%20Cycle%20User%20Study%20Framework) for this work. Zach Cutler presented the paper on the main stage in Vienna.  \\n\\n![Zach Cutler presenting the paper at IEEE VIS in Vienna.](/img/blog_posts/2026_zach_presenting.jpg)\\n\\n\\n\\nThis recognition reflects years of iterative development, community feedback, tutorials, documentation work, and, most importantly, the researchers who have trusted ReVISit in their own studies.\\n\\nReVISit 2 is not the endpoint. It is infrastructure for a research community that increasingly relies on sophisticated, reproducible, browser-based experiments. We look forward to continuing to build it together."},{"id":"/2025/10/27/release-2.3","metadata":{"permalink":"/blog/2025/10/27/release-2.3","source":"@site/blog/2025-10-27-release-2.3.md","title":"ReVISit v2.3: Databases, Screen Capture, Qualitative Analysis, Styling, and More!","description":"Today we\'re announcing several new reVISit features that were released in versions 2.1, 2.2 and 2.3. Major new features include an option to self-host data, remote screen-capture, a collaborative qualitative coding interface, and other features and improvements we\'re excited to share. For full details on the changes, look at the GitHub releases!","date":"2025-10-27T00:00:00.000Z","tags":[],"readingTime":12.05,"hasTruncateMarker":true,"authors":[{"name":"The ReVISit Team","key":"team","page":null}],"frontMatter":{"layout":"post","title":"ReVISit v2.3: Databases, Screen Capture, Qualitative Analysis, Styling, and More!","authors":["team"]},"unlisted":false,"prevItem":{"title":"ReVISit 2 Paper Wins Best Paper Award at IEEE VIS","permalink":"/blog/2025/11/10/ieee-vis-award"},"nextItem":{"title":"ReVISit v2.0: Making your Studies even more Powerful!","permalink":"/blog/2025/01/20/release-2.0"}},"content":"Today we\'re announcing several new reVISit features that were released in versions 2.1, 2.2 and 2.3. Major new features include an option to **self-host data**, **remote screen-capture**, a **collaborative qualitative coding interface**, and other features and improvements we\'re excited to share. For full details on the changes, [look at the GitHub releases!](https://github.com/revisit-studies/study/releases)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Supabase \u2013 A Self-Hosted Alternative to Firebase [Beta]\\nMaybe the biggest change is added support for Supabase, an open-source, self-hostable alternative to Firebase. Not all researchers can use products such as Firebase due to organizational or legal restrictions (GDPR, etc.). Supabase allows researchers with these restrictions to use reVISit inside an environment they fully control and own. Self-hosting is a bit more hands-on than using Firebase, so we make the setup as easy as possible by providing a docker-compose file to spin up Supabase on your own infrastructure. See [Supabase setup guide](https://revisit.dev/docs/data-and-deployment/supabase/setup/) to learn more.\\n\\nNote: we ran a few studies with Supabase without issues, but we don\u2019t have as much experience with it as we do with Firebase, so we\'re marking this feature as Beta until we have had more time to test with it.\\n\\n## Screen Capture [Beta]\\n\\nSince its initial release, reVISit has included [provenance tracking](https://revisit.dev/docs/designing-studies/provenance-tracking/)  using [Trrack](https://apps.vdl.sci.utah.edu/trrack). Provenance tracking can be very helpful for replaying and analyzing exactly how users interact with stimuli. However, it can also be tedious to setup \u2013 especially for complex visualizations with multiple elements to track, or when working with visualizations you don\u2019t control (e.g. [website components](https://revisit.dev/docs/typedoc/interfaces/WebsiteComponent/)).\\n\\nSo today we\u2019re introducing a flexible alternative for capturing user behavior: screen recording. In your study, you now can ask participants to grant browser permissions to record their screen \u2013 much like screen sharing in video conferencing software \u2013 with recordings stored in the reVISit infrastructure. This approach offers a much lower barrier to setup compared to provenance tracking and can be enabled directly through the reVISit config \u2013 no coding required!\\n\\nThe video shows an example screen recording session that also includes audio recording in a think-aloud protocol.\\n<iframe width=\\"100%\\" height=\\"450\\" src=\\"https://www.youtube.com/embed/QMkObxCfxmE?si=NTZGUZTCCtZg5pE6\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nWe\u2019re finding screen recording to be extremely useful for early-stage piloting, debugging, as well as conducting full studies. In fact, if you have other reVISit capabilities enabled, such as think-aloud or provenance tracking, the screen recording syncs properly with them, making it powerful to analyze provenance as participants experience it. See the [Screen recording setup guide](https://revisit.dev/docs/designing-studies/record-screen/) to learn more. As with Supabase, we\u2019re marking this feature as Beta until we have had more time to test with it.\\n\\n## Qualitative Coding for Study Replays\\n\\nWe\u2019ve added a qualitative analysis and coding platform to pair with our participant analysis and replay. This implementation based on what we described in our [CHI paper](https://dl.acm.org/doi/10.1145/3706598.3714305), with extensive design improvements and additional functionality. The analysis displays the transcribed text for a participant and task, and allows users to edit the transcriptions as well as create and apply codebooks to the transcribed audio. \\n\\nThe video shows an example of audio and video captured in a visualization drawing task, with the researcher applying codes to segments of the transcript. \\n\\n<iframe width=\\"100%\\" height=\\"400\\" src=\\"https://www.youtube.com/embed/Y-nUn0d8xrQ?si=1yqtoxRW3Ygij7nr\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nCodes are available for download as part of the data export, making it easy to integrate qualitative coding into your analysis workflow.\\n\\n## Styling Studies\\nWe introduce new tools for customizing the look of the studies, making it easier to match your study\u2019s design to your research goals. Whether you want to make your study polished or improve accessibility for participants, you can do it all directly with the styling options reVISit provides. Styling can be applied in multiple ways \u2013 from inline styles to external CSS files, depending on the level of customization you need.\\n\\nStyles can be set at the global level for the whole study, at the component level for individual stimuli, or at the response (form element) level for specific questions.\\n\\nWith styles, you can get really into the design weeds. For instance, you can make reVISit feel deeply polished by adjusting the form style study, using clean layouts, different font sizes, and bordered responses. The video shows a study emulating Google Forms\u2019 style:\\n\\n![formStyle](https://revisit.dev/assets/images/style-form-954abe29730c080cb962668c93e4653c.gif)\\n\\nYou can try it out and look at various other examples [here](https://revisit.dev/study/demo-style/QStiN0cvR09ERmlPZzNWNDBFcFlKZz09).\\n\\nAdvanced styling can be applied to make studies more engaging. For example, responses can enlarge text as participants start typing or change background when clicked. These interactions make study more engaging, providing a more user-friendly experience.\\n\\n![advancedStyle](https://revisit.dev/assets/images/style-interactive-7d184d9f19ab4208b8966bea135d5557.gif)\\n\\nStyling allows researchers to create clean, accessible, and enjoyable studies without complicated steps. To see more styling examples, check out the [styling documentation](https://revisit.dev/docs/designing-studies/applying-style).\\n\\n## Live Monitor for Viewing Study Progress\\n\\nWe added a new live monitor to the analytics interface. The monitor maintains a live connection to the database, and the moment participants answer a question, the heatmap and progress are updated automatically. This enables you to track participants\u2019 progress without refreshing the page. For studies that have fixed-length trials, it displays all questions and marks answered questions in green. For studies that have dynamic components, it will show answered questions followed by a question mark since the total number of questions is not pre-determined.  Note that the live monitor is currently only supported in Firebase-based studies. \\n\\nThe video shows the live monitor in action: in the left video, a participant is taking the study, and on the right, the live monitor updates automatically as the participant answers questions. We show both, a static and a dynamic study example.\\n<iframe width=\\"100%\\" height=\\"280\\" src=\\"https://www.youtube.com/embed/hF_nVkJP4IY?si=KeMHr5Vfk5MOJNtS\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n## Study Stages\\n\\nA typical study goes through multiple stages, such as debugging by the developer, pre-pilots with friends and family; pilots with real participants and then finally the main study. In data analysis, you do want to keep these stages separate to avoid mixing pilot data with real data. To simplify this process, we introduce a new stage management feature. \\n\\nYou can now create stages and filter participants by their stage in the analysis interface. Stages are also prominently available in the data that you download from reVISit. By default, the study will be in the DEFAULT stage. Study designers can create new stages and give it a color, which will be assigned to new participants. In the table in the participant\'s view, we added a new column that displayed the stage name with corresponding color.\\n<iframe width=\\"100%\\" height=\\"270\\" src=\\"https://www.youtube.com/embed/bx6Sp_5SbMU?si=XTaMaKqRjP-Y9ogb\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n## Advanced Control Over Study Flow With Dynamic Sequence Blocks\\nDynamic blocks enable a study designer to build studies that require adaptive logic, i.e. the response to a question (e.g., right or wrong) can be used to determine the next (e.g., harder or easier) question.\\n\\nYou can check out a [simple demo](https://revisit.dev/study/demo-dynamic/) here, or look [at a real study on just noticeable differences of correlations we recently ran](https://revisit.dev/replication-studies/).\\n\\nTo learn how to use the dynamic blocks in your studies, see the [documentation](https://revisit.dev/docs/designing-studies/dynamic-blocks/).\\n\\n## Study Summary Page\\nWe introduce a study summary page in the analysis interface. This provides a comprehensive overview of the study data in a single dashboard. The summary is organized in three  sections \u2013 Overview Statistics, Component Statistics, and Response Statistics.\\n\\n![Screenshots of the study summary page showing overview stats, component stats and response stats.](https://revisit.dev/assets/images/study-summary-2bffd9d623321af1cd057043e1e8af3a.png)\\n\\n- **Overview Statistics** show key information about the study, such as the total number of participants, study timeframe, average time spent, and the overall correctness of the study.\\n- **Component Statistics** give details about each component of the study, such as number of participants, average time spent, and correctness.\\n- **Response Statistics** show response level information including response type, answer options, and correctness. \\n\\nThis new summary page will help you understand participants\' answers quickly. To learn more, please see the [documentation](https://revisit.dev/docs/analysis/study-summary/).\\n\\n## New Libraries\\n\\nThe reVISit libraries are a growing collection of predefined study components and validated questionnaires for visualization and HCI research. With just a few lines of code, you can import them directly into your study.\\n\\nWe are excited to introduce several new libraries:\\n\\n### Virtual Chinrest Calibration Library\\nA [virtual chinrest](https://www.nature.com/articles/s41598-019-57204-1) can be used to measure the distance between a participant and their monitor by detecting their blind spot. This [reVISit library implementation](https://revisit.dev/study/library-virtual-chinrest/) was created by [Sheng Long](https://mika-long.github.io/), and is the first community contribution!\\n\\n### Visualization & Literacy Assessments\\n\\nWe have added the [Berlin Numeracy Test](https://revisit.dev/study/library-berlin-num/) (3 versions), [Graph Literacy Scale](https://revisit.dev/study/library-graph-literacy-scale/), [CALVI (Critical Thinking Assessment for Literacy in Visualizations)](https://revisit.dev/study/library-calvi/), and [Adaptive Visualization Literacy Assessment Test (A-VLAT)](https://revisit.dev/study/library-adaptive-vlat/SURHUmJIblo3Uk4xUUhNUHQ1VTRIQT09). The Berlin Numeracy Test (adaptive version) and A-VLAT take advantage of our new dynamic block feature, which supports adaptive logic, so the questions shown depend on participants\u2019 previous answers.\\n\\n### Usability and Workload Measures\\n\\nWe have added the [QUIS (Questionnaire for User Interaction Satisfaction)](https://revisit.dev/study/library-quis/), [SAM (Self-Assessment Manikin for affective reactions](https://revisit.dev/study/library-sam/), and [SMEQ (Subjective Mental Effort Questionnaire)](https://revisit.dev/study/library-smeq/). We\u2019ve also updated [NASA-TLX](https://revisit.dev/study/library-nasa-tlx/) by adding the Source of Workload Evaluation, as recommended in the original NASA TLX manual.\\n\\n### Screen Recording Component\\nWe also added a component that [asks for screen recording permissions](https://revisit.dev/study/library-screen-recording/) as a library. This is in support of the screen recording feature discussed above.\\n\\n## New Components and Component Features\\n\\n### Video Stimulus\\nIn 2.1, we\u2019ve added a video stimulus component, allowing participants to view video directly within the study. The video component supports both local videos or external video, with an option to force participants to watch the video which is useful to ensure that they watched the video before proceeding to answer the question. You can also add a timeline to the video to allow participants to skip to specific parts of the video. For more information please see the [docs](https://revisit.dev/docs/designing-studies/image-video-stimulus/#video-stimuli).\\n\\n### Ranking Widget\\nIn 2.3, we\u2019ve added the ranking widget, which allows participants to order or group items. This ranking component is useful for study designers who want to capture participants\u2019 priorities, preferences, or how they categorize different items.\\nThere are three types of ranking widgets that you can use depending on your study goals.\\n- **Ranking Sublist** lets participants rank items in a single list, which is useful when you want them to focus on the relative order of their choices.\\n- **Ranking Categorical** enables participants to rank items within high, medium, and low categories, which is useful when you want to understand which category an item belongs to rather than looking into the precise order.\\n- **Ranking Pairwise** lets participants rank items by comparing them in high and low pairs, instead of ordering every item at once.\\nTo learn more about how to use a ranking widget, refer to our [docs](https://revisit.dev/docs/typedoc/interfaces/RankingResponse/).\\n\\n### Multiselect Dropdown\\nThe multiselect dropdown response became available in version 2.3. Previously, reVISit only offered a standard dropdown that allowed participants to select a single option. The multiselect dropdown enables participants to select multiple options. A dropdown response will automatically render as a multiselect dropdown if you define min or max selections in the configuration. Learn more about dropdown response in the [docs](https://revisit.dev/docs/typedoc/interfaces/DropdownResponse/).\\n\\n### Random Order of Form Elements and Options\\nIn 2.2, we added a randomization feature for form elements and options to help reduce bias and improve the quality of study results. This feature allows individual questions, answer choices, or entire pages of questions to show up in random order for each participant, which is also recorded for replay. For more details on using randomization in studies, visit our [docs](https://revisit.dev/docs/designing-studies/forms/#randomization-of-form-elements).\\n\\n### Previous Buttons\\nPreviously, participants were not allowed to go back in the study to change or correct their answers. In version 2.2, we added a previous button that allows participants to navigate back to earlier questions, if the study designer allows them to review and make corrections to their answers.\\n\\n## New Configuration Options and Analysis Interface Updates\\n\\n### Screen Size Restrictions\\nReVISit now allows you to set minimum screen size requirements to ensure participants have enough space to view study content and complete tasks. When a participant starts a study, reVISit checks their browser size and shows a warning page until the participant resizes their browser. If the requirements are not met within a time frame, the participant is automatically rejected. To learn more about how to apply screen size restriction to your study, please refer to the [documentation](https://revisit.dev/docs/designing-studies/device-restrictions/).\\n\\n### Analysis Audio, Screen Recording and Transcript (Firebase) Download\\nReVISit lets researchers capture and analyze participants\' audio and screen recordings. [Audio and transcripts](https://revisit.dev/docs/analysis/data-export/#download-audio), and [screen recordings](https://revisit.dev/docs/analysis/data-export/#download-screen-recording) can now be downloaded for all participants or selected participants from the Participant View or Replay where previously you had to access these sources via Firebase."},{"id":"/2025/01/20/release-2.0","metadata":{"permalink":"/blog/2025/01/20/release-2.0","source":"@site/blog/2025-01-20-release-2.0.md","title":"ReVISit v2.0: Making your Studies even more Powerful!","description":"It\'s the start of a new year and we\'re excited to announce the release of reVISit v2.0 \u2014 just in time for your VIS 2025 submissions! We\'ve been working hard to bring you a new and improved version of reVISit, and we can\'t wait for you to try it out.","date":"2025-01-20T00:00:00.000Z","tags":[],"readingTime":6.66,"hasTruncateMarker":true,"authors":[{"name":"The ReVISit Team","key":"team","page":null}],"frontMatter":{"layout":"post","title":"ReVISit v2.0: Making your Studies even more Powerful!","authors":["team"]},"unlisted":false,"prevItem":{"title":"ReVISit v2.3: Databases, Screen Capture, Qualitative Analysis, Styling, and More!","permalink":"/blog/2025/10/27/release-2.3"},"nextItem":{"title":"ReVISit v1.0: Taking Control of Your Online Studies!","permalink":"/blog/2024/06/20/release-1.0"}},"content":"It\'s the start of a new year and we\'re excited to announce the release of reVISit v2.0 \u2014 just in time for your VIS 2025 submissions! We\'ve been working hard to bring you a new and improved version of reVISit, and we can\'t wait for you to try it out.\\n\\n{/* truncate */}\\n\\nThere are a lot of new features in this release, so let\'s dive in and take a look at what\'s new:\\n\\n## Feature Highlights\\n\\n### Participant Replay\\n\\nEver wondered where your participants clicked when they completed your study? We\'ve enabled participant replays, so you can now replay the interactions of participants in your studies during analysis. This enables you to see how participants are interacting with your study, either **to discover issues in a pilot**, or to actually **analyze interaction behavior**.\\n\\n<iframe width=\\"100%\\" height=\\"450\\" src=\\"https://www.youtube.com/embed/wjP35gra9J4?si=aNkDA-uN2U7ryVeu\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nCheck out the [demo](https://revisit.dev/study/example-brush-interactions/VTJGc2RHVmtYMS9Qbm9DalhDeS81NVFkamZ1dm11NW41c0hwZUM5ZnZ3UT0=?participantId=8a8aa43e-9914-459e-b0ed-078e28f34504) and the [documentation](https://revisit.dev/docs/analysis/participant-replay/). To enable replay, your study stimulus has to [track provenance](https://revisit.dev/docs/designing-studies/provenance-tracking/).\\n\\n### Vega and Vega-Lite Support\\n\\nWe\'ve added compatibility for [Vega](https://vega.github.io/vega/) and [Vega-Lite](https://vega.github.io/vega-lite/) visualizations as a component type, so you can now include these types of visualizations in your studies, leveraging the power of the reVISit platform. We also support tracking interactions in Vega visualizations, enabling you to inspect and analyze how participants used your stimulus with the aforementioned participant replay.\\n<iframe width=\\"100%\\" height=\\"450\\" src=\\"https://www.youtube.com/embed/_fIrIEidN54?si=9DMpeAcTdvhLETLN\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nCheck out the [demo](https://revisit.dev/study/demo-vega/), and [example of a replay](https://revisit.dev/study/demo-vega/VTJGc2RHVmtYMThjdTNyZXJobjdaSTlUVWpDb3NVckVXbmxNbFR6aWpXRT0=?participantId=6d9b3732-9c90-456e-a22b-652a562bf2e4) and the [documentation](https://revisit.dev/docs/designing-studies/vega-stimulus/).\\n\\n### Recording Participant Audio\\n\\nWe\'ve added support for recording participant audio, enabling you to run **think-aloud studies**\u2014even in crowdsourced settings. This is a great way to gain insight into participants\' thought processes and decision-making strategies and represents our latest effort to support qualitative research in reVISit. Audio recordings are automatically transcribed and part of your regular data download. You can even listen to the audio while watching the interactions of your participants [play out](https://revisit.dev/study/test-audio/VTJGc2RHVmtYMS8rS2dYblRlZDFOVmVrNjRWRVRjKzVTWGhPZzhkb1lzND0=?participantId=04eac92d-9892-4688-8bcc-15cf95145d9b).\\n<iframe width=\\"100%\\" height=\\"450\\" src=\\"https://www.youtube.com/embed/YNXIn-1qsk8?si=cvGRFDna8eJYlKN6\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nCheck out the [demo](https://revisit.dev/study/demo-vega/) and the [documentation](https://revisit.dev/docs/designing-studies/vega-stimulus/).\\n\\n### Libraries\\n\\nShould we test for color blindness? What are our participants\u2019 visualization literacy scores? How do participants rate the aesthetics of a visualization? We commonly ask these and similar questions, and often we use validated existing forms or methodologies to answer them. Re-implementing components is time-consuming and error prone. To address this, we\'ve added support for **libraries**, so you can leverage prebuilt study components to create your own studies.\\n\\nLibraries can save time and effort when creating studies, as you can reuse components that have already been created by others. You can also share your own components with the community by creating your own library and submitting a pull request.\\n<iframe width=\\"100%\\" height=\\"450\\" src=\\"https://www.youtube.com/embed/r95GZKwU5go?si=6Ea1JGdUw7XEgwDD\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nAt launch, we have implemented [nine libraries, ranging from simple checks, to questionnaires, to visualization literacy tests](https://revisit.dev/docs/category/libraries/). Check out the [demos](https://revisit.dev/study/?tab=Libraries) and the [documentation](https://revisit.dev/docs/designing-studies/plugin-libraries/).\\n\\n### Python Bindings\\n\\nWriting JSON can be difficult. Who wants to deal with a study config with 20,000 lines? To make it easier to create large and complex studies, we\'ve implemented Python bindings for the reVISit spec,  reVISitPy, that allow you to interact with reVISit programmatically. Here\u2019s a [basic example ](https://github.com/revisit-studies/revisitpy-examples/blob/main/example_simple_html/example_simple_html.ipynb) of how that works.\\n\\nThere are many promising things you can do with reVISitPy:\\n\\nFirst, we implemented a widget that lets you **run the study you created from inside the Jupyter notebook**. Now you have a fast way of inspecting experiments, randomization, etc.\\n\\nNext, you can **run through the study and download the data you generated straight into your notebook**, so that you can immediately see whether you\u2019re actually collecting all the data you need, and even pilot your analysis!\\n\\n<iframe width=\\"100%\\" height=\\"450\\" src=\\"https://www.youtube.com/embed/8Nad_oSNgKY?si=uTfbBtN-fmMx_wvZ\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nFinally, with the Python bindings we also enable you to design arbitrarily complex studies from **permutations**. What does that mean? Say, you have a stimulus, such as a scatterplot, that can render any compatible dataset, and you want to test how good participants are at judging correlations. You might want to feed in hundreds of different datasets that you\u2019re automatically generating. If you were to do this in the reVISit JSON spec, it would be very painful, because you have to create components for every single stimulus. In reVISitPy, it\u2019s just a few lines of code. And of course, this isn\u2019t limited to passing data into a component, you can permute over tasks, visual stimuli, or even phrasings of your questions, etc.\\n\\nCheck out the [documentation](https://revisit.dev/docs/revisitpy/), the [examples](https://github.com/revisit-studies/revisitpy-examples), and the [reVISitPy repository](https://github.com/revisit-studies/revisitpy) to learn more about how to use it.\\n\\n## Other Features / Changes\\n\\n- **Improved User Interface**: We\'ve redesigned the user interface to make it more intuitive and user-friendly. You\'ll find it even more pleasant to use, with a cleaner and more modern look.\\n- **Forms** have gotten some attention. See the [demo](https://revisit.dev/study/demo-survey/) and the [documentation](https://revisit.dev/docs/designing-studies/forms/):\\n   * We introduced new matrix form elements, which are useful if you want to ask e.g., Likert questions for many different items.\\n   * Forms look nicer after an aesthetics overhaul.\\n   * We introduced dividers to section forms.\\n   * You can allow \u201cI don\u2019t know\u201d as an option for most form elements.\\n   * You can allow \u201cOther\u201d options for checkboxes and radios.\\n   * You can choose horizontal and vertical layouts for checkboxes and radios.\\n   * You can now [design trainings](https://revisit.dev/docs/designing-studies/answers-trainings/) where participants can validate answers\\n-  Data export has improved, including things like participant numbers, clean time (time on task minus task where the browser tab was not active), etc.\\n\\nThese new features represent several months of work from the reVISit team, and we\u2019re excited to share them with the community. We\u2019re aiming to make reVISit more versatile, powerful, and easy to use. As always, we welcome your feedback and ideas for how we can support new directions for research in visualization and interactive systems. The best way to get in touch is to join our [Slack Team](https://join.slack.com/t/revisit-nsf/shared_invite/zt-25mrh5ppi-6sDAL6HqcWJh_uvt2~~DMQ)!\\n\\nWe\u2019re also ready to go on the road and meet you at your institution, or offer a virtual workshop! We recently visited Georgia Tech\u2019s GVU center to give a hands-on overview of reVISit. Catch our upcoming workshop at [CHI in Japan](https://chi2025.acm.org/). **Please reach out if you\u2019re interested in learning more about reVISit or potentially hosting a workshop.**"},{"id":"/2024/06/20/release-1.0","metadata":{"permalink":"/blog/2024/06/20/release-1.0","source":"@site/blog/2024-06-20-release-1.0.md","title":"ReVISit v1.0: Taking Control of Your Online Studies!","description":"Diagram of the revisit workflow. The study specification and components are used to compile an interactive, web-based study. As participants complete the study data is stored in Firebase and can be downloaded as tabular or JSON files, for subsequent analysis in analytics tools.","date":"2024-06-20T00:00:00.000Z","tags":[],"readingTime":7.64,"hasTruncateMarker":true,"authors":[{"name":"The ReVISit Team","key":"team","page":null}],"frontMatter":{"layout":"post","title":"ReVISit v1.0: Taking Control of Your Online Studies!","authors":["team"]},"unlisted":false,"prevItem":{"title":"ReVISit v2.0: Making your Studies even more Powerful!","permalink":"/blog/2025/01/20/release-2.0"}},"content":"[![Diagram of the revisit workflow. The study specification and components are used to compile an interactive, web-based study. As participants complete the study data is stored in Firebase and can be downloaded as tabular or JSON files, for subsequent analysis in analytics tools.](https://vdl.sci.utah.edu/assets/images/posts/2024-06-20_revisit-overview.png)](https://vdl.sci.utah.edu/assets/images/posts/2024-06-20_revisit-overview.png)\\n\\n<br />\\n\\nYou might have heard of **[reVISit](https://revisit.dev/)** before from [our paper](https://vdl.sci.utah.edu/publications/2023_shortpaper_revisit/), or you might have [seen a talk or participated in a meetup](https://revisit.dev/community/#community-activities). But as of today, we\u2019re excited to give the community a chance to run your own studies with reVISit with our 1.0 release \u2013 and CHI is just around the corner!\\n\\n{/* truncate */}\\n\\n## What is reVISit?\\n\\nReVISit is a software framework that enables you to [assemble experimental stimuli and survey questions into an online user study](https://revisit.dev/docs/getting-started/how-does-it-work/).\\nOne of the biggest time-saving features of reVISit is a JSON grammar, the **reVISit Spec**, used to describe the setup of your study.\\nStimuli are contained in components and can be either markdown, images, web pages, React components, or survey questions.\\nThe figure at the top shows the relationship of the reVISit Spec, the components, and how they are then compiled into a study.\\n\\nDue to the different types of components, you can use reVISit for a diverse set of studies, spanning simple surveys, image-based perceptual experiments, and experiments evaluating complex interactive visualizations.\\n\\nReVISit is designed to accommodate sophisticated stimuli and study designs. Suppose you want to [replicate the seminal Cleveland and McGill study](https://revisit.dev/study/demo-cleveland/). With reVISit you could implement a React-based set of visualizations (a bar chart, a stacked bar chart, a pie chart), and then pass parameters, such as the data, and the markers to highlight the marks, all via the study configuration.\\n\\nSimilarly, the reVISit spec enables designers to create [controlled sequences](https://revisit.dev/docs/designing-studies/study-sequences/) defining in which order participants see stimuli. reVISit supports fixed, random, and Latin square designs that can be nested at various levels. For example, the overall study sequence (intro, training, experiment, survey) could be fixed. In the experiment arm, two conditions could use a Latin-square design. Within each condition, the experiment could randomly draw a small number of stimuli from a large stimuli pool while interspersing attention checks at random points and adding breaks.\\n\\n### Assembling and Deploying your Study\\n\\nThe components and your study configuration are then used to [assemble a web-based study](https://revisit.dev/docs/getting-started/your-first-study/). You can first look at your study on your local browser, and if you want to share it deploy it to the web server of your choice. We [recommend and document deploying to GitHub pages](https://revisit.dev/docs/data-and-deployment/deploying-to-static-website/), but any web server you have access to will do.\\n\\nYou can then use the online version to direct participants to your study. You can use crowdsourcing platforms such as Prolific, Mechanical Turk or LabintheWild, or you can simply send a link to participants that you have recruited in other ways.\\n\\nA typical study will have response fields, such as a text field or a slider, to provide the response. Such form-based responses are tracked by reVISit by default and can be downloaded in either JSON or a tidy tabular format. Similarly, you can provide [response data out of interactive stimuli](https://revisit.dev/docs/designing-studies/html-stimulus/). For example, if a task is to click on a specific bar in a bar chart, you can log which bars were clicked. ReVISit tracks a diverse set of browser window events such as mouse moves, clicks, scrolls, resizes, which are time-stamped and can hence be used for basic log file analysis.\\n\\nReVISit also supports advanced provenance tracking based on [Trrack](https://apps.vdl.sci.utah.edu/trrack) a provenance tracking library  developed at our lab. If you instrument your study stimuli with Trrack, you can recreate every state of your interface of every single participant! This can be incredibly useful to [understand nuances of user behavior](https://vdl.sci.utah.edu/publications/2021_chi_revisit/), as well as to help you debug your stimuli by exploring what went wrong in a particular session. In a future release, reVISit will also allow you to dynamically browse these events and fully \u201cre-hydrate\u201d all participants experiments.\\n\\n### Data Storage\\n\\nReVISit is implemented as a (mostly) server-less application, meaning that you don\u2019t have to run, secure, and maintain a server to use reVISit. The only exception to this is data storage, as obviously, the data of online participants has to be stored somewhere.\\n\\nIf you\u2019re running a local study, you can get away without this \u2013 you can just download the data from your browser after a study is complete. For online studies, we use Google Firebase to store data.\\n\\nCurrently, [setting up Firebase for a reVISit study](https://revisit.dev/docs/data-and-deployment/firebase/setup/) might be the most challenging part of working with reVISit. On the plus side, Firebase is a tried-and-true system where you have full control over your data. You even have options to choose the locale of your server so that you are compliant with your country\'s regulations on data storage.\\n\\n### Data Analysis\\n\\nReVISit is not meant to replace your usual data analysis approaches. Instead, it aims to make it easy to export data in the formats you might use in R, Python, or your analysis platform of choice.\\n\\nReVISit, however, does provide a basic [analytics interface](https://revisit.dev/docs/analysis/) that is most useful for monitoring the progress of your study. You can also use reVISit to identify participants that didn\u2019t appropriately complete the study and reject them, which is most useful if you want to ensure that you have appropriate numbers of participants in your Latin square design.\\n\\n## What are the Benefits of Using reVISit?\\n\\nSo, why would you use reVISit over other approaches to running your study, such as Qualtrics, Survey Monkey, or even a custom experiment interface?\\n\\nFirst, **reVISit is open source** with all the benefits you have of using open source software: it\u2019s free; you can extend it; you can contribute to improving it.\\n\\nSecond, the open source nature and our approach of forking reVISit for your own study and storing your data in your own Firebase means that **you have full control over your study and the data**. Once you have forked the study, it will remain accessible and unchanged for as long as you like.\\n\\nThird, reVISit has dedicated modes for **quickly navigating your study**, and you can also turn off data collection. This is great for both, developing your study, but also sharing your study with reviewers and readers of your research. That means that readers can see **exactly** what your participants saw, and hence may trust your study more. They could also fork your study and run a **replication of your study** with minimal effort! You can check out an [example study and the associated results.](https://vdl.sci.utah.edu/viz-guardrails-study/)\\n\\n## I\u2019m Intrigued, but Can I Trust it for my Experiment?\\n\\nreVISit is new, and we know that it\u2019s fraught to bet on a new project if you don\u2019t know whether it actually works or whether it will be maintained down the line. But we hope we can convince you to trust us!\\n\\nFirst, we currently have multiple years of funding to continue development of reVISit.\\nWe\u2019ve also ourselves run several successful studies, such as [a study on guardrails against misinformation](https://vdl.sci.utah.edu/viz-guardrails-study/). Finally, we are committed to help you out if you run into issues! Join our [slack team](https://join.slack.com/t/revisit-nsf/shared_invite/zt-25mrh5ppi-6sDAL6HqcWJh_uvt2~~DMQ) to get low-friction help, or write to us at [contact@revisit.dev](mailto:contact@revisit.dev). We\u2019re also happy to set up a meeting to answer any questions you may have; for example, to talk us through whether reVISit will work for your study design.\\n\\n## How Can I Learn More or Get Involved?\\n\\nWe\u2019re grateful to all the community members who have shared their study needs and helped to make reVISit 1.0 a reality, and we\u2019re looking forward to bringing the community exciting new features in the coming year. Future releases will include better debugging tools through study rehydration, a way to capture and code think-aloud data, and improved analysis capabilities. Depending on community feedback we\'re also interested in branching out to unconventional display devices (phones, AR/VR, etc.)\\n\\nTo take your first steps with reVISit, check out our [getting started guide](https://revisit.dev/docs/getting-started/) for instructions on how to install our software and to build a study.\\n\\nFinally, if you are missing a feature or find a bug, let us know! Since reVISit is completely open source you could even submit a pull request!\\n\\n## Acknowledgements\\n\\nWe are very grateful to everyone who helped make reVISit a reality, including our wonderful [community advisory board](https://revisit.dev/community/#community-advisory-board) and the [National Science Foundation for generous funding](https://vdl.sci.utah.edu/projects/2022-nsf-revisit/)."}]}}')}}]);